---
title: "Homework 2 - P8105 (UNI: gvs2113)"
output: github_document
author: Grace Santos 
date: "2023-09-27"
---


```{r}
library(tidyverse)
```

## Problem 1 

The following problem was attempted (as shown in previous commits and pushes) but I changed the code to better reflect what was given as the correct solution after it was posted. I did this in order to have record of how to do the problem correctly when it is time to review my homework for studying purposes. 

```{r, pols_month}
 month_df = 
    tibble(
    month_num = 1:12, 
    month_abb = month.abb, 
    month = month.name)

pols_month_df = 
  read_csv("fte_data/pols-month.csv") |> 
  separate(mon, into = c("year", "month_num", "day"), convert = TRUE) |> 
  mutate(
    president = recode(prez_gop, "0" = "dem", "1" = "gop", "2" = "gop")) |> 
  left_join(x = _, y = month_df) |> 
  select(year, month, everything(), -day, -starts_with("prez"))
```

```{r, snp}
snp_df = 
  read_csv("fte_data/snp.csv",
           col_types = cols(date = col_date(format = "%m/%d/%y"))) |> 
  separate(date, into = c("year", "month_num", "day"), convert = TRUE) |> 
  mutate(
    year = if_else(year > 2023, year - 100, year)) |> 
  left_join(x = _, y = month_df) |> 
  select(year, month, close)
```

```{r, unemployment}
unemploy_df = 
  read_csv("fte_data/unemployment.csv") |> 
  rename(year = Year) |> 
  pivot_longer(
    Jan:Dec,
    names_to = "month_abb",
    values_to = "unemployment"
  ) |> 
  left_join(x = _, y = month_df) |> 
  select(year, month, unemployment)
```

```{r}
fte_data = 
  left_join(pols_month_df, snp_df) |> 
  left_join(x = _, y = unemploy_df)

str(fte_data)
```

When looking at the final joined data, there are `NA` values in the `close` and `unemployment` variables, which indicates missing values. 

The `pols_month_df` data has `r nrow(pols_month_df)` observations and `r ncol(pols_month_df)` variables and tells us about the party affiliation distribution (democrat or republican) for governors and senators for a given year from years `r pols_month_df |> pull(year) |> min()` to `r pols_month_df |> pull(year) |> max()`. It also tells us whether the sitting president was a democrat or republican. 

The `snp_df` data has `r nrow(snp_df)` observations and `r ncol(snp_df)` variables, ranging from years `r snp_df |> pull(year) |> min()` to `r snp_df |> pull(year) |> max()`. 

The `unemploy_df` data has `r nrow(unemploy_df)` observations and `r ncol(unemploy_df)` variables ranging from years `r unemploy_df |> pull(year) |> min()` to `r unemploy_df |> pull(year) |> max()`. 

In Januarys in or after 1975 in which a democrat was president, the **average unemployment rate was `r filter(fte_data, month == "January", year >= 1975, president == "dem") |> pull(unemployment) |> mean() |> round(2)`**. The average unemployment rate over the same time period in which a republican was president was `r filter(fte_data, month == "January", year >= 1975, president == "gop") |> pull(unemployment) |> mean() |> round(2)`.




## Problem 2 
```{r}
mr_trash = 
  readxl::read_excel("trash_data/202309 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", range = "A2:M586") |> 
  janitor::clean_names() |> 
  mutate(
    year = as.numeric(year), 
    name = "mr_trash_wheel",
    homes_powered = (weight_tons *500)/30)

prof_trash = 
    readxl::read_excel("trash_data/202309 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel", range = "A2:L108") |> 
  janitor::clean_names() |> 
  mutate(
    name = "prof_trash_wheel",
    homes_powered = (weight_tons *500)/30)

gwy_trash = 
  readxl::read_excel("trash_data/202309 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel", range = "A2:K157") |> 
  janitor::clean_names() |>
  mutate(
    name = "gwynnda_trash_wheel",
    homes_powered = (weight_tons *500)/30)

trash_tidy = 
  bind_rows(mr_trash, prof_trash, gwy_trash) |> 
  select(name, everything())
```

The Mr. Trash data has `r nrow(mr_trash)` observations and `r ncol(mr_trash)` variables and tells us about the types, weight, and volume of trash collected on specific dates from a water-wheel vessel (named Mr. Trash Wheel) located in the Inner Harbor in Baltimore, MD.  

In an effort to further reduce the amount of trash pollution in Baltimore, the Healthy Harbor initiative has included three more water-wheel vessels named: Professor Trash Wheel, Captain Trash Wheel and Gwynnda Trash Wheel. In this problem, the data for the Professor and Gwynnda trash wheels were examined. The Professor data has `r nrow(prof_trash)` observations and `r ncol(prof_trash)` variables and the Gwynnda data has `r nrow(gwy_trash)` observations and `r ncol(gwy_trash)` variables.

When all three data sets are joined together in the `trash_tidy` dataframe, there are a total of `r nrow(trash_tidy)` observations and `r ncol(trash_tidy)` variables. From years `r prof_trash |> pull(year) |> min()` to `r prof_trash |> pull(year) |> max()`, Professor Trash Wheel collected a total of `r filter(trash_tidy, name == "prof_trash_wheel") |> pull(weight_tons) |> sum() |> round(2)` tons of trash. Gwynnda Trash Wheel collected a total of `r filter(trash_tidy, name == "gwynnda_trash_wheel", month == "July", year == "2021") |> pull(cigarette_butts) |> sum()` cigarette butts in July of 2021. 




## Problem 3 
```{r}
bl_data = 
  read_csv("mci_data/MCI_baseline.csv", skip = 1, col_names = TRUE, na = c(".", "NA")) |> 
  janitor::clean_names() |> 
  mutate(
    sex = 
      case_match(
        sex, 
        0 ~ "female",
        1 ~ "male"),
    apoe4 = 
      case_match(
        apoe4, 
        0 ~ "non-carrier",
        1 ~ "carrier")) |> 
  filter(current_age < age_at_onset | is.na(age_at_onset))
```

It was important to look at the original csv for baseline data before importing to  get an idea of what I needed to include for the needs of this problem. Once I observed that the first row had explanatory, but otherwise had unnecessary information, I decided to use the `skip` argument in my `read_csv()` function to not include it. It was also necessary to keep the column names with the `col_names = TRUE` argument and edit the data to interpret the "." in the age at onset column as `NA` using the `na = "."` argument. It was helpful to then clean the column names into snake case and then proceed with encoding the sex and APOE4 carrier status and filtering out those participants who did not have have MCI at baseline. This was done by filtering by ages, if the participants age at onset was less than their current age, that meant that they had the MCI at baseline and needed to be excluded. 

In total, there were `r nrow(bl_data)` participants recruited and `r filter(bl_data, age_at_onset != "NA") |> nrow()`  participants developed MCI. The average baseline age was `r bl_data |> pull(current_age) |>  mean() |> round(0)` years old. Within the total the study participant group, there are  `r (filter(bl_data, sex == "female" & apoe4 == "carrier") |> nrow()) / (nrow(bl_data)) * 100` percent of women who are APOE4 carriers. 

```{r}
amy_data = 
  read_csv("mci_data/mci_amyloid.csv", skip = 1, col_names = TRUE) |> 
  janitor::clean_names() |> 
  rename("id" = "study_id") |> 
  pivot_longer(
    baseline:time_8,
    names_to = "time_elapsed(years)",
    values_to = "amyloid_ratio"
    ) 
```


It was also important to look at the original amyloid csv file before importing it so that I would know to include the `skip` and the `col_names = TRUE` arguments within my `read_csv()` function. I then cleaned the names and renamed the "study_id" column name to just "id" so that it was consistent with the column naming used in the baseline data. Also, since the amyloid data set gave measurements of the amyloid ratio computed at different time points, I decided to use the `pivot_longer()` function to further tidy the dataset so that time and the ratio can be separate columns. While this did increase the length of the data set tremendously, it did help to simplify the readability of the dataset and make it more tidy. After all of the tidying steps, overall the amyloid data had a total of `r nrow(amy_data)` rows with `r ncol(amy_data)` columns. 


In attempting to check the participants who only appeared in the baseline or amyloid datasets, I found a resource online that utilized the `comparedf()` function within the `"arsenal"` package to be able to run a detailed summary of differences between two datasets. I installed the package and compared the  `bl_data` and `amy_data` data sets by ID numbers using the code below. 

```{r, eval = FALSE}
library(arsenal)
summary(comparedf(bl_data, amy_data, by = "id"))
```

After running the code, the output resulted in finding 8 participants only in the baseline data (ID# 14, 49, 92, 179, 268, 304, 389, 412) and 16 participants who were only in the amyloid data (ID# 72, 234, 283, 380, 484-495). 


```{r}
mci_tidy = 
 inner_join(bl_data, amy_data)

write.csv(mci_tidy, "./mci_data/combined_data.csv", row.names = TRUE)
```

Lastly, both data sets were joined by ID number using the `inner_join()` function. This resulted in a datset with `r nrow(mci_tidy)` rows and `r ncol(mci_tidy)` columns. The completed data file can be found in my mci_data folder in my data directory. 


